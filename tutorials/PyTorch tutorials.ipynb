{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad6b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5302170",
   "metadata": {},
   "source": [
    "# 1. Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbee282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensors are specialized data structures similar to arrays and matrices. \n",
    "# use tensors to encode inputs and outputs of models, as well as model parameters \n",
    "# tensors can run on GPUs or other specialized hardware to accelerate computing. \n",
    "\n",
    "\n",
    "# tensor from list of lists\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117b1af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor from np array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a70858e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9472, 0.0728],\n",
      "        [0.6206, 0.8476]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tensor from another tensor\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91616b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am familiar with numpy, so the operations between tensors should be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a7411",
   "metadata": {},
   "source": [
    "# 2. torch.autograd: forward and backward propagation\n",
    "- Neural Networks are a collection of nested functions that are executed on some input data. \n",
    "- Functions are defined by parameters, (with weights and biases), and are stored as tensors in pytorch \n",
    "## 2b. Training a neural network\n",
    "1. First step is <b> FORWARD PROPAGATION </b>\n",
    "    - In this step, the neural network makes its best guess about the correct output. NN runs input data through each of its functions to make this guess\n",
    "    \n",
    "2. Second Step is <b> BACKWARD PROPAGATION </b>\n",
    "    - In this step, NN adjusts its parameters proportionate to the error in its guess. It will traverse backwards from the output, collecting derivatives of the error with respect to the parameters of the functions. And will optimize the parameters using gradient descent. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f430672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "# 1. Model is a pretrained resnet18 from torch vision\n",
    "# 2. data is a random data tensor to represent a single image with 3 channels, with height and width of 64. \n",
    "# 3. label of the image is random as well. \n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "# 4. run the forward pass\n",
    "prediction = model(data) # forward pass\n",
    "\n",
    "# 5. calculate the error \n",
    "loss = (prediction - labels).sum()\n",
    "\n",
    "# 6. backprop the error through the network\n",
    "loss.backward() #backward pass\n",
    "\n",
    "# 7. load an optimizer. SGD is used in this case.\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# 8. gradient descent -- to adjust each parameter by the gradient stored. in each parameter\n",
    "optim.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9401af",
   "metadata": {},
   "source": [
    "# 3. Neural Networks\n",
    "\n",
    "- Neural Networks can be constructe using the torch.nn package. \n",
    "- nn depends on autograd to define models and differentiate them. \n",
    "- nn.Module contains layers, and a forward(inut) method that returns the output. \n",
    "- typical training procedure for nn:\n",
    "    1. Define the neural network that has some learnable parameters (weights)\n",
    "    2. Iterate over a dataset of inputs\n",
    "    3. Process input through the network\n",
    "    4. Compute the loss (how far is the output from being correct)\n",
    "    5. Propagate gradients back into the network's parameters\n",
    "    6. Update the weights of the network, typically using a simple update rule: <br>\n",
    "    `weight = weight - learning_rate * gradient`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719127b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the network: \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        # color, output channels, square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # max pool over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # if the size is a square, we can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        # flatten all dimensions except the batch dimension \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1ac31",
   "metadata": {},
   "source": [
    "- we only have to define the forward function\n",
    "- the backward function (GD) is automatically defined for us using autograd. \n",
    "- we can use any of the tensor operations in the forward function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f335657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view learnable parameters of a model by: net.parameters()\n",
    "params = list(net.parameters())\n",
    "params[0].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0861c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0312,  0.0379,  0.0165, -0.0343, -0.0931,  0.0614, -0.0580,  0.0550,\n",
      "          0.1098,  0.0340]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# trying a random 32x32 input. FORWARD PROPAGATION\n",
    "# expected input size of the net is 32x32. We need to resize images to 32x32 if we wanna test this net on\n",
    "## another dataset. \n",
    "\n",
    "x_test = torch.randn(1, 1, 32,32)\n",
    "out = net(x_test)\n",
    "print(out)\n",
    "\n",
    "# zero the gradient buffers of all parameters, and backprop with random gradients\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f4cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6965, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = net(x_test)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "## COMPUTING THE LOSS\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54408d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x7ff438c7ee20>\n",
      "<AddmmBackward0 object at 0x7ff438c7ee50>\n",
      "<AccumulateGrad object at 0x7ff438c7ee20>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(loss.grad_fn)  # MSELoss \n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "befe2824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0030,  0.0067,  0.0099, -0.0030,  0.0076,  0.0048])\n"
     ]
    }
   ],
   "source": [
    "## BACKPROP\n",
    "\n",
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "## must zero grad or else the gradients will be accumulated to existing gradients. \n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720a080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPDATE WEIGHTS/ Optimizer\n",
    "# we can use torch.optim, look into this.\n",
    "\n",
    "# stochastic gradient descent is the simplest update rule\n",
    "# weight = weight - learning_rate * gradient\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee3dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
